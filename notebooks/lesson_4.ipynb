{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lección IV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### _<font color=blue>  Objetivo de la lección: </font>_\n",
    "\n",
    "<font color=blue>\n",
    "  Manejo Masivo de Datos: Pyspark. \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introducción\n",
    "\n",
    "**Apache Spark** se está convirtiendo rápidamente en una de las mejores plataformas de análisis de datos de código abierto.\n",
    "\n",
    "Con la mejora continua de **Apache Spark**, especialmente el motor **SQL** y el surgimiento de proyectos relacionados,  estamos comenzando a obtener la funcionalidad de análisis de datos que teníamos en configuraciones de una sola máquina utilizando RDBMS y bibliotecas de análisis de datos como **Pandas**.\n",
    "\n",
    "**Pandas**, una herramienta de análisis de datos para el lenguaje de programación Python, es actualmente la herramienta de análisis de datos abierta más popular y madura. La biblioteca está altamente optimizada para el rendimiento, con rutas de código críticas escritas en Cython o C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pandas vs Spark\n",
    "\n",
    "Hemos usado **Pandas** para ordenar  nuestros datos de manera directa, facil y eficiente. Pandas nos ha ayudado a limpiar, transformar, manipular y analizar los datos. El punto de introducir **Spark**, es que se vuelve mejor que  **Pandas** cuando la cantidad de datos no es _grande_ sino _gigante_.\n",
    "\n",
    "Hay una gran diferencia entre la cantidad de  datos grande (_Large_)  y la cantidad de datos gigante (_Big_). Y aunque estas medidas relativas algunos autores llama _big data_ a datos que ocupan mas de 100 GB.\n",
    "\n",
    "Los numeros exactos de cuando usar uno o el otro varian dependiendo del tipo de version de software y hardware con que se cuenta. Aunque algunas pruebas concluyen que conjunto de datos con menos de 10 millones de filas (<5 GB de archivo) no se debe analizar con Spark, mientras que otras sugieren usar Spark solo cuando los datos sobrepasan los 200 GB. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ¿Por qué usar PySpark?\n",
    "\n",
    "Al utilizar Spark, la mayoría de los ingenieros de datos recomienda desarrollarlo en Scala (que es el lenguaje Spark \"nativo\") o en Python a través de la API completa de PySpark.\n",
    "\n",
    "Hemos visto que Python es flexible, robusto, fácil de aprender y se beneficia de la gran cantidad de bibliotecas que hay. Para nosotros Python es el lenguaje perfecto para la creación de prototipos en los campos de Big Data / Machine Learning.\n",
    "\n",
    "Al igual que en las lecciones pasadas implementamos un proyecto concreto  de ciencia de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " ##  Accidentes cerebrovasculares: El conjunto de datos\n",
    " \n",
    " Spark es un proyecto de código abierto de Apache. También es el motor de análisis más utilizado para el big data y el aprendizaje automático.\n",
    "\n",
    "Esta lección se centrará en un inicio rápido para desarrollar un algoritmo de predicción con Spark.\n",
    "\n",
    "Elegí el conjunto de datos \"Healthcare Dataset Stroke Data\" para trabajar con kaggle.com, la comunidad de científicos de datos y aprendizaje automático más grande del mundo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Accidentes cerebrovasculares\n",
    "\n",
    "Según la Organización Mundial de la Salud, la cardiopatía isquémica y el accidente cerebrovascular representan las mayores causas de muerte en el mundo.\n",
    "\n",
    "Información del sitio oficial: http://www.who.int/news-room/fact-sheets/detail/the-top-10-causes-of-death\n",
    "\n",
    "Lo que debemos hacer es predecir la probabilidad de accidente cerebrovascular utilizando la información dada de los pacientes. Es un problema de clasificación, en el que intentaremos predecir la probabilidad de que una observación pertenezca a una categoría (en nuestro caso, la probabilidad de sufrir un accidente cerebrovascular).\n",
    "\n",
    "Como hemos visto existe una amplia gamma de para resolver los problemas de clasificación. Nos restringimos al algoritmo de Árbol de decisiones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Configurando Spark y cargando los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Para correr en **Pyspark** usamos uno de los\n",
    "__[contenedores de docker](https://github.com/jupyter/docker-stacks)__ para Spark y Python. \n",
    "\n",
    "_Docker es una herramienta diseñada para facilitar la creación, implementación y ejecución de aplicaciones mediante el uso de contenedores._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql as sparksql\n",
    "spark = SparkSession.builder.appName('stroke').getOrCreate()\n",
    "entrena = spark.read.csv('../data/train_2v.csv', inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exploración de datos\n",
    "La primera operación que se realiza después de importar datos es obtener información sobre su aspecto. Se puede hacer con los siguientes comandos:\n",
    "\n",
    "_df.printSchema()_\n",
    "\n",
    "_df.describe()_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      " |-- hypertension: integer (nullable = true)\n",
      " |-- heart_disease: integer (nullable = true)\n",
      " |-- ever_married: string (nullable = true)\n",
      " |-- work_type: string (nullable = true)\n",
      " |-- Residence_type: string (nullable = true)\n",
      " |-- avg_glucose_level: double (nullable = true)\n",
      " |-- bmi: double (nullable = true)\n",
      " |-- smoking_status: string (nullable = true)\n",
      " |-- stroke: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "entrena.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, id: string, gender: string, age: string, hypertension: string, heart_disease: string, ever_married: string, work_type: string, Residence_type: string, avg_glucose_level: string, bmi: string, smoking_status: string, stroke: string]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entrena.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|stroke|count|\n",
      "+------+-----+\n",
      "|     1|  783|\n",
      "|     0|42617|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "entrena.groupBy('stroke').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Como se puede ver en esta observación. Este es un conjunto de datos desequilibrado, donde el número de observaciones que pertenecen a una clase es significativamente menor que las que pertenecen a las otras clases. En este caso, el modelo predictivo podría ser parcial e inexacto. Existen diferentes estrategias para manejar los conjuntos de datos desequilibrados. Para encontrar más información sobre el conjunto de datos desequilibrado:\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2017/03/imbalanced-classification-problem/\n",
    "\n",
    "Aquí tenemos mediciones clínicas (por ejemplo, hipertensión, enfermedad cardíaca, edad, antecedentes familiares de la enfermedad) para varios pacientes, así como información sobre si cada paciente ha tenido un accidente cerebrovascular. En la práctica, queremos que este método prediga con precisión el riesgo de accidente cerebrovascular para futuros pacientes en función de sus mediciones clínicas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img style=\"float: center;\" src=\"../images/kaggle-table.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spark SQL\n",
    "Spark SQL es un módulo de Spark para el procesamiento de datos estructurados. Internamente, Spark SQL utiliza esta información adicional para realizar optimizaciones adicionales. \n",
    "\n",
    "### SQL\n",
    "Un uso de Spark SQL es ejecutar consultas SQL. Spark SQL también se puede utilizar para leer datos de una instalación existente de Hive. Para obtener más información sobre cómo configurar esta función, consulte la sección Tablas de Hive. Al ejecutar SQL desde otro lenguaje de programación, los resultados se devolverán como un conjunto de datos / marco de datos. También puede interactuar con la interfaz SQL utilizando la línea de comandos o sobre JDBC / ODBC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spark SQL\n",
    "\n",
    "### Dataframes\n",
    "\n",
    "Un DataFrame es un Dataset organizado en columnas con nombre. Es similar al Dataframe de **Pandas**. Conceptualmente es equivalente a una tabla en una base de datos relacional o un marco de datos en R / Python, pero con optimizaciones más ricas. Los DataFrames se pueden construir a partir de una amplia gama de fuentes, tales como: archivos de datos estructurados, tablas en Hive, bases de datos externas o RDD existentes. \n",
    "\n",
    "_**PySpark** y **Pandas** refieren su estructura de datos como 'DataFrames' pero son diferentes plataformas en tiempo de ejecución._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Análisis\n",
    "\n",
    "Realizar un breve análisis utilizando operaciones básicas. Es posible hacerlo de varias maneras:\n",
    "\n",
    "Los DataFrames proporcionan un lenguaje específico del dominio para la manipulación de datos estructurados, el acceso a las columnas de un DataFrame puede ser por atributo o por indexación para ejecutar consultas SQL mediante programación y devolver el resultado como un DataFrame\n",
    "\n",
    "Por ejemplo, para ver qué tipo de trabajo tiene más casos de accidentes cerebrovasculares, podemos hacer lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# crear un DataFrame como una vista temporal\n",
    "entrena.createOrReplaceTempView('table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Análisis Exploratorio: Consultas SQL\n",
    "\n",
    "En spark podemos hecer nuestros analisis exploratorios de los datos mediante consultas SQL  (expresiones formales en el lenguaje SQL).\n",
    "\n",
    " SQL es un lenguaje de administración de bases de datos para bases de datos relacionales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+\n",
      "|    work_type|work_type_count|\n",
      "+-------------+---------------+\n",
      "|      Private|            441|\n",
      "|Self-employed|            251|\n",
      "|     Govt_job|             89|\n",
      "|     children|              2|\n",
      "+-------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT work_type, count(work_type) as work_type_count \\\n",
    "           FROM table WHERE stroke == 1 GROUP BY work_type \\\n",
    "           ORDER BY work_type_count DESC\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Parece que la ocupación privada es el tipo de trabajo más peligroso en este conjunto de datos.\n",
    "\n",
    "Averigüemos quiénes participaron en esta medición clínica. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+-------------------+\n",
      "|gender|count_gender|            percent|\n",
      "+------+------------+-------------------+\n",
      "|Female|       25665|  59.13594470046083|\n",
      "| Other|          11|0.02534562211981567|\n",
      "|  Male|       17724|  40.83870967741935|\n",
      "+------+------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT gender, count(gender) as count_gender,\\\n",
    "           count(gender)*100/sum(count(gender)) over() as percent \\\n",
    "           FROM table GROUP BY gender\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " El 59% de todas las personas son mujeres y solo el 40% son hombres que participaron en la investigación de accidentes cerebrovasculares:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A partir de esta información, existe la posibilidad de recuperar información sobre la cantidad de mujeres / hombres que tienen un ataque (cerebrovascular). La siguiente funcion nos devuelve el porcentaje de un genero dado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def porcentaje_ataque(genero='Male'):\n",
    "    genero = \"'\"+genero+\"'\"    \n",
    "    spark.sql(\"SELECT gender, count(gender),\\\n",
    "           (COUNT(gender) * 100.0) /(SELECT count(gender) \\\n",
    "            FROM table WHERE gender == \"+genero+\") as percentage \\\n",
    "            FROM table WHERE stroke = '1' \\\n",
    "            and gender = \"+genero+\" GROUP BY gender\").show()\n",
    "    return    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+----------------+\n",
      "|gender|count(gender)|      percentage|\n",
      "+------+-------------+----------------+\n",
      "|  Male|          352|1.98600767321146|\n",
      "+------+-------------+----------------+\n",
      "\n",
      "+------+-------------+----------------+\n",
      "|gender|count(gender)|      percentage|\n",
      "+------+-------------+----------------+\n",
      "|Female|          431|1.67932982661212|\n",
      "+------+-------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "porcentaje_ataque('Male')\n",
    "porcentaje_ataque('Female')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1,68% Mujeres y casi 2% Hombres han tenido un accidente cerebrovascular.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "También podemos ver si la edad influye en el accidente cerebrovascular y cuál es el riesgo por edad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "| age|age_count|\n",
      "+----+---------+\n",
      "|79.0|       70|\n",
      "|78.0|       57|\n",
      "|80.0|       49|\n",
      "|81.0|       43|\n",
      "|82.0|       36|\n",
      "|70.0|       25|\n",
      "|74.0|       24|\n",
      "|77.0|       24|\n",
      "|76.0|       24|\n",
      "|67.0|       23|\n",
      "|75.0|       23|\n",
      "|72.0|       21|\n",
      "|69.0|       20|\n",
      "|59.0|       20|\n",
      "|68.0|       20|\n",
      "|71.0|       19|\n",
      "|57.0|       19|\n",
      "|63.0|       18|\n",
      "|65.0|       18|\n",
      "|66.0|       17|\n",
      "+----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT age, count(age) as age_count \\\n",
    "          FROM table WHERE stroke == 1 GROUP BY age \\\n",
    "          ORDER BY age_count DESC\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Podemos usar la operación de filtro para calcular el número de casos de accidente cerebrovascular para personas después de 50 años."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "708"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entrena.filter((entrena['stroke'] == 1) & (entrena['age'] > '50')).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Como podemos ver, la edad es un factor de riesgo importante para desarrollar un derrame cerebral."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Limpieza de Datos\n",
    "El siguiente paso de la exploración es tratar con los valores categóricos y los valores faltantes. Hay valores faltantes para smoking_status y bmi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Rellenamos smoking_status con un valor de 'Sin información'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "entrena_f = entrena.na.fill('No Info', subset=['smoking_status'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " Rellenamos el parámetro bmi con el valor medio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "promedio = entrena_f.select(mean(entrena_f['bmi'])).collect()\n",
    "promedio_bmi = promedio[0][0]\n",
    "entrena_f = entrena_f.na.fill(promedio_bmi,['bmi'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La mayoría de los algoritmos ML no pueden trabajar directamente con datos categóricos. La codificación permite que los algoritmos que esperan que las características continuas usen características categóricas.\n",
    "\n",
    "StringIndexer -> OneHotEncoder -> VectorAssembler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import (VectorAssembler,OneHotEncoder,\n",
    "                                StringIndexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "indexador_genero = StringIndexer(inputCol='gender', outputCol='genderIndex')\n",
    "codificador_genero = OneHotEncoder(inputCol='genderIndex', outputCol='genderVec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "indexador_casado = StringIndexer(inputCol='ever_married', outputCol='ever_marriedIndex') \n",
    "codificador_casado = OneHotEncoder(inputCol='ever_marriedIndex', outputCol='ever_marriedVec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "indexador_tipo_trabajo = StringIndexer(inputCol='work_type', outputCol='work_typeIndex')\n",
    "codificador_tipo_trabajo = OneHotEncoder (inputCol='work_typeIndex', outputCol='work_typeVec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "indexador_residencia = StringIndexer(inputCol='Residence_type', outputCol='Residence_typeIndex') \n",
    "codificador_residencia = OneHotEncoder (inputCol='Residence_typeIndex', outputCol='Residence_typeVec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "indexador_status_fumador = StringIndexer(inputCol='smoking_status', outputCol='smoking_statusIndex')\n",
    "codificador_status_fumador = OneHotEncoder (inputCol='smoking_statusIndex', outputCol='smoking_statusVec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "No es necesario saber cuántas categorías hay de antemano  en una característica, la combinación de StringIndexer y OneHotEncoder se encargan de ello.\n",
    "\n",
    "El siguiente paso es crear un ensamblador, que combine una lista dada de columnas en una columna de un solo vector para entrenar el modelo ML. Usamos las columnas de vectores, que obtuvimos después de OneHotEncoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ensamblador = VectorAssembler(inputCols=['genderVec',\n",
    " 'age',\n",
    " 'hypertension',\n",
    " 'heart_disease',\n",
    " 'ever_marriedVec',\n",
    " 'work_typeVec',\n",
    " 'Residence_typeVec',\n",
    " 'avg_glucose_level',\n",
    " 'bmi',\n",
    " 'smoking_statusVec'],outputCol='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Luego crearemos un objeto DecisionTree. Para hacer esto necesitamos importar DecisionTreeClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "dtc = DecisionTreeClassifier(labelCol='stroke',featuresCol='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Hasta ahora tenemos una tarea compleja que contiene un montón de etapas, que deben realizarse para procesar los datos. Para envolver todo eso, Spark ML representa un flujo de trabajo como Pipeline, que consiste en una secuencia de PipelineStages que se ejecutarán en un orden específico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[indexador_genero, indexador_casado,\\\n",
    "                            indexador_tipo_trabajo, indexador_residencia, \n",
    "                            indexador_status_fumador,\\\n",
    "                            codificador_genero, codificador_casado, \\\n",
    "                            codificador_tipo_trabajo, codificador_residencia, \\\n",
    "                            codificador_status_fumador , \\\n",
    "                            assembler, dtc \\\n",
    "                           ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El siguiente paso es dividir el conjunto de datos para entrenar y probar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "entrena_data,prueba_data = entrena_f.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Lo que voy a hacer ahora es ajustar el modelo. Para esto usaré el pipeline que fue creado y train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "modelo = pipeline.fit(entrena_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Después de eso transforma los test_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dtc_predicciones = modelo.transform(prueba_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Es ahora, el momento de evaluar un modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Decision Tree algorithm had an accuracy of: 98.04%\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "# Select (prediction, true label) and compute test error\n",
    "acc_evaluator = MulticlassClassificationEvaluator(labelCol=\"stroke\", predictionCol=\"prediction\",\\\n",
    "                                                  metricName=\"accuracy\")\n",
    "dtc_acc = acc_evaluator.evaluate(dtc_predicciones)\n",
    "print('A Decision Tree algorithm had an accuracy of: {0:2.2f}%'.format(dtc_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Un algoritmo de Árbol de Decisión tenía una precisión de: 98.04%\n",
    "\n",
    "Como se definió al principio, el modelo predictivo de un conjunto de datos desequilibrado podría ser con una precisión engañosa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Conclusiones\n",
    "Hemos discutido las ventajas y desventajas de usar Pyspark o Pandas.  Hemos trabajado el ducto de ciencias de datos para el conjunto de datos para el caso de accidentes cerebrovasculares"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
