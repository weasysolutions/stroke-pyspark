{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cuarta Lección "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Spark está escrito en el lenguaje de programación Scala que compila el código del programa en un código de bytes para la Máquina Virtual de Java para el procesamiento de big data de spark. La comunidad de código abierto ha desarrollado una maravillosa utilidad para el procesamiento de big data de spark python conocido como PySpark.\n",
    "\n",
    "PySpark ayuda a los científicos de datos a interactuar con los Datos Resilientes Distribuidos (Resilient Distributed Datasets) en apache spark y python. Py4J es una biblioteca popular integrada en PySpark que permite a Python interactuar dinámicamente con objetos JVM (RDD).\n",
    "\n",
    "La mayoría de las operaciones de RDD son perezosas. Un RDD es como una descripción de una serie de operaciones. Un RDD no son datos. Las operaciones RDD que requieren la observación del contenido de los datos no pueden ser perezosas. (Estas son llamadas acciones.) Por ejemplo, la funcion count() que enumera el numero de elementos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el procesamiento paralelo, Apache Spark utiliza variables compartidas. Una copia de la variable compartida va a cada nodo del clúster cuando el controlador envía una tarea al ejecutor en el clúster, de modo que pueda usarse para realizar tareas.\n",
    "\n",
    "Apache Spark admite dos tipos de variables compartidas:\n",
    "\n",
    " __Emisión__\n",
    "\n",
    "__Acumulación__\n",
    "\n",
    "Vamos a entenderlos en detalle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emisión\n",
    "Las variables de difusión se utilizan para guardar la copia de datos en todos los nodos. Esta variable se almacena en caché en todas las máquinas y no se envía en máquinas con tareas. El siguiente bloque de código tiene los detalles de una clase de transmisión para PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "from pyspark import SparkContext \n",
    "sc = SparkContext(\"local\", \"Broadcast app\") \n",
    "words_new = sc.broadcast([\"scala\", \"java\", \"hadoop\", \"spark\", \"akka\"]) \n",
    "data = words_new.value \n",
    "print(\"Stored data -> %s\" % (data) )\n",
    "elem = words_new.value[2] \n",
    "print(\"Printing a particular element in RDD -> %s\" % (elem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consideremos el siguiente ejemplo de uso de SparkConf en un programa PySpark. En este ejemplo, estamos configurando el nombre de la aplicación de la chispa como aplicación PySpark y estableciendo la URL maestra para una aplicación de la chispa en → spark: // master: 7077.\n",
    "\n",
    "El siguiente bloque de código tiene las líneas, cuando se agregan en el archivo Python, establece las configuraciones básicas para ejecutar una aplicación PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "conf = SparkConf().setAppName(\"PySpark App\").setMaster(\"spark://master:7077\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark import SparkFiles\n",
    "finddistance = \"/home/hadoop/examples_pyspark/finddistance.R\"\n",
    "finddistancename = \"finddistance.R\"\n",
    "sc = SparkContext(\"local\", \"SparkFile App\")\n",
    "sc.addFile(finddistance)\n",
    "print \"Absolute Path -> %s\" % SparkFiles.get(finddistancename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<font color=blue>\n",
    "    _Objetivo del curso_: Aprender las diferentes fases del Aprendizaje de Datos (ML) a traves de  ejemplos reales. Esto con el fin de obtener una idea clara de lo que significa la ciencia de datos.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align=\"center\">El flujo de trabajo en un proyecto de ciencia de datos </h2>\n",
    "\n",
    "El conjunto de  lineamientos específicos ('ducto') en un proyecto de  ciencia de datos varía dependiendo de la naturaleza del mismo. Aquí presentamos un ducto estandar:\n",
    "\n",
    "\n",
    "### Analisis exploratorio de los datos\n",
    "\n",
    " - Extracción: Cargar el conjunto de datos y echarles una mirada\n",
    " - Limpieza: Encontrar los valores que faltan\n",
    " - Visualización: Crear algunas gráficas interesantes que nos permitan idendificar correlaciones y \n",
    " - Suposiciones: Formular hipótesis sobre los gráficos\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ingeniería de Características\n",
    "\n",
    " - Datos Categóricos\n",
    " - Extraer\n",
    " - Procesar\n",
    " \n",
    "### Modelado de datos\n",
    " - Selección de Características.  Reducir el número de caracterísiticas trae los siguientes beneficios:\n",
    "     - Reduce el número de redundancias en los datos\n",
    "     - Acelera los procesos de entrenamiento \n",
    "     - Reduce el 'overfitting'\n",
    "     - Muestreo de modelos base\n",
    " - Muestreo de los modelos base   \n",
    " ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "_Ajustamos un poco el estilo de este libro trabajo para tener graficas centradas_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".output_png {\n",
    "    display: table-cell;\n",
    "    text-align: center;\n",
    "    vertical-align: middle;\n",
    "}\n",
    "</style>\n",
    "\"\"\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caracteristicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "caracteristicas =  ['a:radio','b:textura','c:perimetro', \\\n",
    "             'd:area', 'e:suavidad','f:compactes', \\\n",
    "             'g:concavidad', 'h:puntos_concavos','i:simetria',\\\n",
    "             'j:dimension_fractal']\n",
    "\n",
    "columnas = ['ID', 'Diagnostico'] + \\\n",
    "              list(map(lambda x: x[2:] + '_promedio', caracteristicas)) + \\\n",
    "              list(map(lambda x: x[2:] + '_error', caracteristicas)) +  \\\n",
    "              list(map(lambda x: x[2:] + '_peor', caracteristicas))      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisis exploratorio de los datos\n",
    "Resumen de la lección pasada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/wisconsin.csv', names = columnas);\n",
    "data = data.reset_index().drop(columns =['index'])\n",
    "data = data.drop(columns='ID')\n",
    "\n",
    "data['Benigno'] = (data['Diagnostico']=='B')*1\n",
    "data['Maligno'] = 1 - data['Benigno']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ingeniería de Características\n",
    "\n",
    "**Datos Categóricos** \n",
    "\n",
    "Los datos categóricos son variables que contienen valores de etiqueta en lugar de valores numéricos. El número de valores posibles a menudo se limita a un conjunto fijo.\n",
    "\n",
    "Por ejemplo, los usuarios normalmente se describen por país, género, grupo de edad, etc.\n",
    "\n",
    "De hecho ya hemos  convertido los atributos del _Diagnóstico_ a valores numéricos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Maligno</th>\n",
       "      <th>Benigno</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Maligno  Benigno\n",
       "101        0        1\n",
       "445        0        1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[['Maligno','Benigno']].sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Las columnas _Maligno_ y _Benigno_ son redundates. El conocimiento de una, automaticamente nos define la otra. Eliminamos una:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data = data.drop(columns = 'Maligno')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En este punto la columna _Diagnostico_ tambien se vuelve redundante, por lo que también la eliminamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data = data.drop(columns = 'Diagnostico')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**División el conjunto de datos** \n",
    "\n",
    "Los datos que utilizamos generalmente se dividen en datos de entrenamiento y datos de prueba. \n",
    "El conjunto de entrenamiento contiene _un_ resultado conocido y \n",
    "el modelo aprende sobre estos datos para generalizarse a otros datos más adelante. \n",
    "Tenemos el conjunto de datos de prueba (o subconjunto) para probar la predicción de nuestro modelo \n",
    "en este subconjunto.\n",
    "***\n",
    "Primero, separamos el conjunto total de diagnósticos (presentado por la letra _Y_) del resto de los atributos (representados por la letra _X_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, 0:29].values\n",
    "Y = data.iloc[:, 30].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separamos el conjunto de entrenamiento y el conjunto prueba usando la biblioteca **SciKit-Learn**. Más en especifico, el método 'train_test_split':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El parametro _test_\\__size_ representa el porcentaje de los datos _(X,Y)_  que serán usados como datos de prueba. En nuestro caso hemos usado el veinticinco porciento. El parámetro _random_\\__state_ sirve para establecer la semilla del algoritmo aleatorio que selecciona los conjuntos de datos \n",
    "***\n",
    "### Escalamiento de los atributos \n",
    "\n",
    "La mayoría de las veces, un conjunto de datos contendrá características altamente variables en su magnitudes, unidades y rango. Por ejemplo notemos la gran disparidad que existe entre las columnas 'dimension_fractal' y 'radio_promedio':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>area_promedio</th>\n",
       "      <th>dimension_fractal_promedio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>654.889104</td>\n",
       "      <td>0.062798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>351.914129</td>\n",
       "      <td>0.007060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>143.500000</td>\n",
       "      <td>0.049960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>420.300000</td>\n",
       "      <td>0.057700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>551.100000</td>\n",
       "      <td>0.061540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>782.700000</td>\n",
       "      <td>0.066120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2501.000000</td>\n",
       "      <td>0.097440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       area_promedio  dimension_fractal_promedio\n",
       "count     569.000000                  569.000000\n",
       "mean      654.889104                    0.062798\n",
       "std       351.914129                    0.007060\n",
       "min       143.500000                    0.049960\n",
       "25%       420.300000                    0.057700\n",
       "50%       551.100000                    0.061540\n",
       "75%       782.700000                    0.066120\n",
       "max      2501.000000                    0.097440"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[['area_promedio','dimension_fractal_promedio']].describe() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Asi como la cercania o lejania entre dos ciudades distintas se establece con la distancia que los separa, en estadística también tenemos el concepto de distancia entre los datos _medidos_ con los datos del _modelo teórico_. De hecho, la noción de distancia en estadística es la misma que la que hay para dos ciudades: la medida Euclideana.\n",
    " Con ella, podemos decir si nuestras predicciones del modelo están cerca o lejos de las predicciones de los datos medidos. Es un poco descabellado comparar distancias de ciudades con las distancias entre dos hormigas en un mismo hormiguero, pero los ejemplos del 'area_promedio' y 'dimension_fractal_promedio' arriba expuestos  nos  están demostrando que semejantemente nos encontramos en esta situacion.\n",
    " ***\n",
    " <div style=\"text-align: center\">  _Es deseable llevar todas las características al mismo nivel de magnitudes. Esto se puede lograr mediante la escala. Esto significa transformar todos los datos a una misma  escala específica, \n",
    "como  de 0 a 100 o de  0 a 1._  </div>\n",
    " ***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ello, usamos el metodo _StandardScaler_ de la bilbioteca **sklearn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Modelado de datos\n",
    " - Seleccion de Caracteristicas.  Reducir el numero de caracterisiticas trae los siguientes beneficios:\n",
    "     - Reduce el numero de redundancias en los datos\n",
    "     - Acelera los procesos de entrenamiento \n",
    "     - Reduce el 'overfitting'\n",
    "     - Muestreo de modelos base\n",
    " - Muestreo de los modelos base   \n",
    " ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el area del aprendizaje de maquinas se diferencian las predicciones en dos tipos. Por un lado, las predicciones que toman valores en el  continuo (en la recta real), se les llama _Regresiones_. Por ejemplo, el tiempo que tarda un avion en cruzar el continente. Mientras que las predicciones que invulcran solo un numero finito de predicciones (discreto o categórico), se les conoce como _clasificaciones_. Por ejemplo un conjunto de tres colores, el día o la noche, etc.\n",
    "***\n",
    "En nuestro caso queremos predicir si el cancer es maligno o benigno, entonces usaremos _algoritmos de clasificacion para el aprendizaje supervisado_ \n",
    "***\n",
    "La palabra supervisado, aqui significa que sabemos que etiqueta final queremos predecir: _Benigno_ o _Maligno_. Las etiquetas _Benigno_  y _Maligno_ que usamos para entrenar a la maquina, no deben ser llamadas  _predicciones_ sino _datos de salida_: toda prediccion es un dato de salida pero no al reves.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EN SUMA:\n",
    "En aprendizaje automático y estadísticas, la clasificación consiste identificar a cuál de un conjunto de categorías (subpoblaciones) pertenece una nueva observación, sobre la base de un conjunto de entrenamiento de datos que contienen observaciones (o instancias) cuya categoría de miembro es conocida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los siguientes nombres de algoritmos de clasificación en Aprendizaje Automático, son de los más populares en el mundo de la ciencia de datos:\n",
    "\n",
    "1. Regresión logística\n",
    "\n",
    "2. Vecino más cercano\n",
    "\n",
    "3. Soporte de máquinas de vectores\n",
    "\n",
    "4. Kernel SVM\n",
    "\n",
    "5. Naïve Bayes\n",
    "\n",
    "6. Árbol de decisión\n",
    "\n",
    "7. Bosques al azar\n",
    "\n",
    "La biblioteca **sklearn** tiene en incluído todos estos casos. A continuación describimos cada uno de los métodos e importamos las correspondendientes sub-librerias ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplificamos con la regresion _regresión logística_ el como se realizamos el analisis para despues analizar todos los casos juntos "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REGRESIÓN LOGÍSTICA.\n",
    "\n",
    "En su forma mas sencilla (la cual es la que necesitamos aqui), la regresion logistica modela las variables de salida 'Benigno y 'Maligno, en este caso hablamos de una _regresión logística binaria_\n",
    "\n",
    "__Ensayo de Bernoulli:__\n",
    "Ensayos repetidos independientes de un experimento con exactamente dos resultados posibles se llaman _ensayos de Bernoulli_ (el resultado de un paciente no influyen en los demás)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La regresión logística difiere de la regresión ordinaria en que la segunda regresa valores continuos. En la regresión logísitica, la simplicidad de la regresión lineal es usada, pero tiene que ser adicionada con  una forma de convertir una variable binaria en una continua que pueda tomar cualquier valor real (negativo o positivo). \n",
    "\n",
    "Para ello se definen  la _frontera de decisión_ (_boundary decision_). \n",
    "\n",
    "Esta predicción categórica se puede basar en las probabilidades calculadas de éxito, y las probabilidades pronosticadas por encima de un valor de corte elegido se traducen en una predicción de éxito. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones de Activación\n",
    "\n",
    "Computacionalmente a veces es mejor aproximar probabilidades de valores categóricos con funciones continuas conocidas como funciones de activación.\n",
    "\n",
    "Por ejemplo las funciones _sigmoid_ y _tanh_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-11f70edd28e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):  \n",
    "    return  np.array([1 / (1 + np.exp(-y)) for y in x])\n",
    "\n",
    "def tanh(x):\n",
    "    return np.array([np.tanh(y) for y in x])\n",
    "\n",
    "t = np.linspace(-4,4,400)\n",
    "\n",
    "a = 4*sigmoid(t) - 2\n",
    "b = 1*tanh(t) + 0\n",
    "\n",
    "plt.plot(t,a,t,b)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación cargamos el método de regresión logística de **skit-learn**, para despues hacer el ajuste:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using Logistic Regression Algorithm to the Training Set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression(random_state = 0)\n",
    "\n",
    "#Una vez cargado el clasificador, hacemos el ajuste\n",
    "classifier.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para verificar la precisión de la predicción necesitamos importar el método de confusión de matriz de métricas. La matríz de confusión es una forma de tabular el número de clasificaciones erróneas, es decir, el número de clases predichas que terminaron en un contenedor de clasificación incorrecto basado en las clases verdaderas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matríz de Error o Matríz de Confusión \n",
    "\n",
    "En el análisis predictivo, una tabla de confusión (a veces también llamada matriz de confusión), es una tabla con dos filas y dos columnas que informa el número de _falsos positivos_, _falsos negativos_, _verdaderos positivos_ y _verdaderos negativos_.\n",
    "\n",
    "Cada fila de la matriz representa las instancias en una clase predicha, mientras que cada columna representa las instancias en una clase real (o viceversa). En nuesro caso, tenemos algo como:\n",
    "\n",
    "\n",
    "| Prediccion/Real | Maligno | Benigno |\n",
    "| --- | --- | --- |\n",
    "| Maligno | $VP$ | $FP$|\n",
    "| Benigno | $FN$ | $VN$ |\n",
    "\n",
    "$VP$ = Numero de  Verdadero Positivo\n",
    "\n",
    "$FP$ = Numero de Falso Positivo\n",
    "\n",
    "$VN$ = Numero de Verdadero Negativo\n",
    "\n",
    "$FN$ = Numero de Falso Negativo\n",
    "\n",
    "\n",
    "**condición positiva (P):**\n",
    "El número de casos positivos reales en los datos.\n",
    "\n",
    "**condición negativa (N):**\n",
    "El número de casos negativos reales en los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos el método de métrica que contiene la matriz de confusión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los 143 casos probados nos arrojan la siguiente matriz de confusión: \n",
      " [[49  4]\n",
      " [ 4 86]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "#La matriz de confusión\n",
    "cm = metrics.confusion_matrix(Y_test, Y_pred)\n",
    "\n",
    "print('Los', len(Y_test), 'casos probados', \\\n",
    "      'nos arrojan la siguiente matriz de confusión: \\n', cm \n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exactitud \n",
    "\n",
    "Para el caso de errores categoricos (como en nuestro caso), esta simple clasificacion puede ser engañosa, por que las muestras pueden estar cargadas hacia un solo caso. Es recomendable apoyarnos en la _Exactitud Matematica_, la cual viene dada simplemente por la relacion:\n",
    "***\n",
    "$\\textit{Exactitud} = \\frac{\\text{Número de Predicciones Verdaderas}}{\\text{Número Total de Predicciones}}$\n",
    "\n",
    "\n",
    "Para nuestro caso de predicciones categóricas binarias:\n",
    "\n",
    "\n",
    "$\\textit{Exactitud} = \\frac{\\text{VP+VN}}{\\text{VP+VN+FP+FN}}$\n",
    "***\n",
    "El método _metrics.accuracy_score_ lo calcula por nosotros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9440559440559441"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(Y_test, Y_pred, normalize=True, sample_weight=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toca el turno de repetir el mismo ejercicio sobre las diferentes rutinas de Machine Learning que **sklearn** nos provee. Prim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clasificadores = {}\n",
    "\n",
    "#Para el conjunto de entrenamiento usamos:\n",
    "\n",
    "#Usando el Algoritmo de Regresion Logistica \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clasificadores['Regresion Logistica'] = LogisticRegression(random_state = 0)\n",
    "clasificadores['Regresion Logistica'].fit(X_train, Y_train)\n",
    "\n",
    "#Usando el Algoritmo de K Vecinos Próximos \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clasificadores['K Vecinos Próximos'] = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
    "clasificadores['K Vecinos Próximos'].fit(X_train, Y_train)\n",
    "\n",
    "#Usando el Algoritmo de Máquinas de Soporte Vectorial \n",
    "from sklearn.svm import SVC\n",
    "clasificadores['Máquinas de Soporte Vectorial'] = SVC(kernel = 'linear', random_state = 0)\n",
    "clasificadores['Máquinas de Soporte Vectorial'].fit(X_train, Y_train)\n",
    "\n",
    "#Usando el Algoritmo de Núcleo SVM \n",
    "from sklearn.svm import SVC\n",
    "clasificadores['Núcleo SVM '] = SVC(kernel = 'rbf', random_state = 0)\n",
    "clasificadores['Núcleo SVM '].fit(X_train, Y_train)\n",
    "\n",
    "#Usando el Algoritmo Bayessiano Ingenuo\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clasificadores['Bayes Ingenuo'] = GaussianNB()\n",
    "clasificadores['Bayes Ingenuo'].fit(X_train, Y_train)\n",
    "\n",
    "#Usando el Arbol de Decisión \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clasificadores['Arbol de Decisión'] = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "clasificadores['Arbol de Decisión'].fit(X_train, Y_train)\n",
    "\n",
    "#Usando el Bosque Aleatorio \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clasificadores['Bosque Aleatorio'] = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "clasificadores['Bosque Aleatorio'].fit(X_train, Y_train);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "exactitudes = {}\n",
    "for clasificador in clasificadores:\n",
    "    \n",
    "    Y_pred = clasificadores[clasificador].predict(X_test) \n",
    "    \n",
    "    exactitudes[clasificador] = metrics.accuracy_score(Y_test, Y_pred, normalize=True, sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El clasificador Regresion Logistica es exacto hasta un 94.41 %\n",
      "El clasificador K Vecinos Próximos es exacto hasta un 95.80 %\n",
      "El clasificador Máquinas de Soporte Vectorial es exacto hasta un 96.50 %\n",
      "El clasificador Núcleo SVM  es exacto hasta un 96.50 %\n",
      "El clasificador Bayes Ingenuo es exacto hasta un 92.31 %\n",
      "El clasificador Arbol de Decisión es exacto hasta un 95.10 %\n",
      "El clasificador Bosque Aleatorio es exacto hasta un 95.80 %\n"
     ]
    }
   ],
   "source": [
    "for clasificador in exactitudes:\n",
    "    print('El clasificador', clasificador, 'es exacto hasta un',  '%.2f' % (100*exactitudes[clasificador]),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validación cruzada\n",
    "\n",
    "Mucha veces en vez de dividir inicialmente el conjunto de datos en dos subconjuntos, lo hacemos en tres subconjuntos: _entrenamiento_, _validación_ y _prueba_\n",
    "\n",
    "dejan de lado el conjunto de pruebas y eligen aleatoriamente el X% \n",
    "de su conjunto de datos de tren para que sea el conjunto de trenes real y el restante (100-X)% \n",
    "para ser el conjunto de validación, donde X es un número fijo (por ejemplo, 80%). ), \n",
    "el modelo se entrena y valida iterativamente en estos diferentes conjuntos.\n",
    "Hay varias formas de hacer esto, y se conoce comúnmente como validación cruzada. \n",
    "Básicamente, utiliza su conjunto de entrenamiento para generar múltiples divisiones \n",
    "de los conjuntos de Entrenamiento y Validación. La validación cruzada evita el ajuste \n",
    "excesivo y se está volviendo cada vez más popular, siendo la Validación Cruzada K-fold \n",
    "el método más popular de validación cruzada.\n",
    "Mira esto para más.\n",
    "\n",
    "En el enfoque que estamos siguiendo (también muy común entre los practicantes del aprendizaje de datos) \n",
    "\n",
    "\n",
    "Nosotros estamos usando una forma especifica de _validacio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota sobre la validación cruzada: muchas veces,\n",
    "las personas primero dividen su conjunto de datos en 2 - Train and Test.\n",
    "Después de esto, dejan de lado el conjunto de pruebas y eligen aleatoriamente el X% \n",
    "de su conjunto de datos de tren para que sea el conjunto de trenes real y el restante (100-X)% \n",
    "para ser el conjunto de validación, donde X es un número fijo (por ejemplo, 80%). ), \n",
    "el modelo se entrena y valida iterativamente en estos diferentes conjuntos.\n",
    "Hay varias formas de hacer esto, y se conoce comúnmente como validación cruzada. \n",
    "Básicamente, utiliza su conjunto de entrenamiento para generar múltiples divisiones \n",
    "de los conjuntos de Entrenamiento y Validación. La validación cruzada evita el ajuste \n",
    "excesivo y se está volviendo cada vez más popular, siendo la Validación Cruzada K-fold \n",
    "el método más popular de validación cruzada.\n",
    "Mira esto para más."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las columnas _Maligno_ y _Benigno_ son redundates. El conocimiento de una, automaticamente nos define la otra. Eliminamos una:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
